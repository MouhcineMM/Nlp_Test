{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn6CqEHotEfx",
        "outputId": "760d62e1-9b42-4f2a-dc53-6e05150b2d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_2' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_3' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_3' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_2' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.9301 - f1_score: 0.1591 - loss: 0.1305 - precision: 0.3046 - recall: 0.1153"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_3' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_2' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_f1_score improved from -inf to 0.69404, saving model to models/best_keyword_model_20250626_201852.keras\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 371ms/step - accuracy: 0.9302 - f1_score: 0.1604 - loss: 0.1302 - precision: 0.3063 - recall: 0.1163 - val_accuracy: 0.9680 - val_f1_score: 0.6940 - val_loss: 0.0532 - val_precision: 0.7586 - val_recall: 0.6396 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.9766 - f1_score: 0.7895 - loss: 0.0441 - precision: 0.8046 - recall: 0.7751\n",
            "Epoch 2: val_f1_score improved from 0.69404 to 0.72211, saving model to models/best_keyword_model_20250626_201852.keras\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 338ms/step - accuracy: 0.9766 - f1_score: 0.7895 - loss: 0.0441 - precision: 0.8046 - recall: 0.7751 - val_accuracy: 0.9700 - val_f1_score: 0.7221 - val_loss: 0.0511 - val_precision: 0.7610 - val_recall: 0.6870 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - accuracy: 0.9814 - f1_score: 0.8349 - loss: 0.0367 - precision: 0.8538 - recall: 0.8169\n",
            "Epoch 3: val_f1_score did not improve from 0.72211\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 331ms/step - accuracy: 0.9814 - f1_score: 0.8349 - loss: 0.0367 - precision: 0.8538 - recall: 0.8169 - val_accuracy: 0.9702 - val_f1_score: 0.7207 - val_loss: 0.0539 - val_precision: 0.7674 - val_recall: 0.6792 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.9863 - f1_score: 0.8743 - loss: 0.0281 - precision: 0.8879 - recall: 0.8611\n",
            "Epoch 4: val_f1_score did not improve from 0.72211\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 336ms/step - accuracy: 0.9862 - f1_score: 0.8742 - loss: 0.0281 - precision: 0.8879 - recall: 0.8610 - val_accuracy: 0.9696 - val_f1_score: 0.7207 - val_loss: 0.0564 - val_precision: 0.7520 - val_recall: 0.6919 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.9883 - f1_score: 0.8953 - loss: 0.0241 - precision: 0.9075 - recall: 0.8835\n",
            "Epoch 5: val_f1_score did not improve from 0.72211\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 340ms/step - accuracy: 0.9883 - f1_score: 0.8953 - loss: 0.0242 - precision: 0.9075 - recall: 0.8835 - val_accuracy: 0.9686 - val_f1_score: 0.7060 - val_loss: 0.0602 - val_precision: 0.7525 - val_recall: 0.6649 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.9907 - f1_score: 0.9166 - loss: 0.0191 - precision: 0.9309 - recall: 0.9028\n",
            "Epoch 6: val_f1_score did not improve from 0.72211\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 333ms/step - accuracy: 0.9907 - f1_score: 0.9166 - loss: 0.0191 - precision: 0.9308 - recall: 0.9028 - val_accuracy: 0.9668 - val_f1_score: 0.7148 - val_loss: 0.0711 - val_precision: 0.6967 - val_recall: 0.7339 - learning_rate: 5.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.9921 - f1_score: 0.9306 - loss: 0.0165 - precision: 0.9365 - recall: 0.9249\n",
            "Epoch 7: val_f1_score did not improve from 0.72211\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 326ms/step - accuracy: 0.9921 - f1_score: 0.9306 - loss: 0.0165 - precision: 0.9365 - recall: 0.9249 - val_accuracy: 0.9675 - val_f1_score: 0.7056 - val_loss: 0.0760 - val_precision: 0.7252 - val_recall: 0.6870 - learning_rate: 5.0000e-04\n",
            "Epoch 7: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_3' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_2' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_3' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_2' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv_1' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "from typing import List, Tuple, Dict, Any\n",
        "from pathlib import Path\n",
        "\n",
        "# TensorFlow/Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Dense, Dropout, TimeDistributed, BatchNormalization,\n",
        "    Conv1D, LSTM, MultiHeadAttention, Concatenate, GlobalMaxPooling1D,\n",
        "    Bidirectional, Add, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "import difflib\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration optimisée pour le modèle NER (extraction de mots-clés)\"\"\"\n",
        "    def __init__(self):\n",
        "        # Data paths\n",
        "        self.parquet_files = [\n",
        "            \"validation-00000-of-00001 (1).parquet\",\n",
        "            \"test-00000-of-00001.parquet\",\n",
        "            \"train-00000-of-00001.parquet\"\n",
        "        ]\n",
        "        self.columns_keep = ['text', 'keywords', 'topic']\n",
        "        self.model_save_path = Path(\"models/\")\n",
        "        self.logs_path = Path(\"logs/\")\n",
        "        self.results_path = Path(\"results/\")\n",
        "\n",
        "        # NER parameters (optimisés pour mots-clés)\n",
        "        self.test_size = 0.15\n",
        "        self.val_size = 0.15\n",
        "        self.random_state = 42\n",
        "        self.vocab_size = 15000  # Réduit pour éviter l'overfitting\n",
        "        self.oov_token = \"<OOV>\"\n",
        "        self.pad_token = \"<PAD>\"\n",
        "        self.embedding_dim = 128  # Réduit\n",
        "        self.filters = [64, 128, 64]  # Réduit\n",
        "        self.kernel_sizes = [2, 3, 4]  # Ajusté\n",
        "        self.dropout_rate = 0.3  # Réduit\n",
        "        self.l1_reg = 1e-5\n",
        "        self.l2_reg = 1e-4\n",
        "        self.batch_size = 16  # Réduit\n",
        "        self.epochs_ner = 10  # Augmenté\n",
        "        self.learning_rate = 1e-3  # Augmenté\n",
        "        self.patience_early_stopping = 5  # Réduit\n",
        "        self.patience_lr_reduce = 3\n",
        "        self.lr_factor = 0.5\n",
        "        self.min_lr = 1e-6\n",
        "\n",
        "        # Paramètres d'optimisation\n",
        "        self.use_bidirectional = True\n",
        "        self.use_attention_ner = False  # Désactivé pour simplifier\n",
        "        self.gradient_clip_norm = 1.0\n",
        "\n",
        "        # Nouveaux paramètres pour améliorer la correspondance\n",
        "        self.similarity_threshold = 0.8\n",
        "        self.min_keyword_length = 2\n",
        "        self.max_sequence_length = 128  # Réduit\n",
        "\n",
        "        # Create directories\n",
        "        for path in [self.model_save_path, self.logs_path, self.results_path]:\n",
        "            path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Gestion des données pour extraction de mots-clés depuis Parquet\"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.ner_tokenizer = None\n",
        "        self.nlp = self._load_spacy_model()\n",
        "        self.class_weights = None\n",
        "        self.df_combined = None\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        \"\"\"Chargement sécurisé du modèle spaCy\"\"\"\n",
        "        try:\n",
        "            return spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "        except OSError:\n",
        "            try:\n",
        "                return spacy.load(\"fr_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "            except OSError:\n",
        "                logger.warning(\"Aucun modèle spaCy trouvé, utilisation de tokenisation basique\")\n",
        "                return None\n",
        "\n",
        "    def load_parquet_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Chargement des fichiers Parquet\"\"\"\n",
        "        logger.info(\"Chargement des fichiers Parquet...\")\n",
        "\n",
        "        # Vérifier l'existence des fichiers\n",
        "        existing_files = []\n",
        "        for file in self.config.parquet_files:\n",
        "            if Path(file).exists():\n",
        "                existing_files.append(file)\n",
        "            else:\n",
        "                logger.warning(f\"Fichier non trouvé: {file}\")\n",
        "\n",
        "        if not existing_files:\n",
        "            raise FileNotFoundError(\"Aucun fichier Parquet trouvé\")\n",
        "\n",
        "        # Lire et combiner les fichiers\n",
        "        dfs = []\n",
        "        for file in existing_files:\n",
        "            try:\n",
        "                df = pd.read_parquet(file)[self.config.columns_keep]\n",
        "                dfs.append(df)\n",
        "                logger.info(f\"Chargé {file}: {len(df)} lignes\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erreur lors du chargement de {file}: {e}\")\n",
        "\n",
        "        self.df_combined = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        # Nettoyer les données NaN\n",
        "        self.df_combined = self.df_combined.dropna(subset=['text', 'keywords'])\n",
        "\n",
        "        logger.info(f\"Dataset combiné: {len(self.df_combined)} lignes\")\n",
        "        return self.df_combined\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Nettoyage du texte\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Garder plus de ponctuation pour une meilleure tokenisation\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\(\\)\\'\\\"]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def parse_keywords(self, keywords_str: str) -> List[str]:\n",
        "        \"\"\"Parse la chaîne de mots-clés pour extraire une liste\"\"\"\n",
        "        if pd.isna(keywords_str) or not isinstance(keywords_str, str):\n",
        "            return []\n",
        "\n",
        "        keywords_str = keywords_str.strip()\n",
        "\n",
        "        # Si c'est une liste Python en string\n",
        "        if keywords_str.startswith('[') and keywords_str.endswith(']'):\n",
        "            try:\n",
        "                import ast\n",
        "                return ast.literal_eval(keywords_str)\n",
        "            except:\n",
        "                keywords_str = keywords_str[1:-1]\n",
        "\n",
        "        # Diviser par virgule et nettoyer\n",
        "        keywords = [kw.strip().strip('\"\\'') for kw in keywords_str.split(',')]\n",
        "        keywords = [kw for kw in keywords if kw and len(kw) >= self.config.min_keyword_length]\n",
        "        return keywords\n",
        "\n",
        "    def tokenize_sentence(self, sentence: str) -> List[str]:\n",
        "        \"\"\"Tokenisation d'une phrase\"\"\"\n",
        "        if not sentence.strip():\n",
        "            return []\n",
        "\n",
        "        if self.nlp is not None:\n",
        "            try:\n",
        "                doc = self.nlp(sentence)\n",
        "                return [token.text.lower() for token in doc if not token.is_space and not token.is_punct]\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Erreur lors de la tokenisation spaCy: {e}\")\n",
        "\n",
        "        # Tokenisation basique\n",
        "        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "        return [token for token in tokens if len(token) >= 2]\n",
        "\n",
        "    def find_keyword_matches(self, tokens: List[str], keywords: List[str]) -> List[int]:\n",
        "        \"\"\"Trouve les correspondances entre tokens et mots-clés avec fuzzy matching\"\"\"\n",
        "        labels = [0] * len(tokens)\n",
        "\n",
        "        for keyword in keywords:\n",
        "            keyword_lower = keyword.lower().strip()\n",
        "            keyword_tokens = self.tokenize_sentence(keyword_lower)\n",
        "\n",
        "            if not keyword_tokens:\n",
        "                continue\n",
        "\n",
        "            # Correspondance exacte\n",
        "            for i, token in enumerate(tokens):\n",
        "                if token in keyword_tokens or any(kt in token for kt in keyword_tokens):\n",
        "                    labels[i] = 1\n",
        "\n",
        "            # Correspondance floue pour les mots-clés composés\n",
        "            if len(keyword_tokens) > 1:\n",
        "                keyword_text = ' '.join(keyword_tokens)\n",
        "                token_text = ' '.join(tokens)\n",
        "\n",
        "                # Chercher des sous-séquences\n",
        "                for i in range(len(tokens) - len(keyword_tokens) + 1):\n",
        "                    window = ' '.join(tokens[i:i+len(keyword_tokens)])\n",
        "                    similarity = difflib.SequenceMatcher(None, window, keyword_text).ratio()\n",
        "\n",
        "                    if similarity >= self.config.similarity_threshold:\n",
        "                        for j in range(i, i + len(keyword_tokens)):\n",
        "                            if j < len(labels):\n",
        "                                labels[j] = 1\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def create_ner_dataset(self, df: pd.DataFrame) -> Tuple[List, List]:\n",
        "        \"\"\"Créer un dataset NER à partir du DataFrame\"\"\"\n",
        "        sentences = []\n",
        "        labels = []\n",
        "\n",
        "        logger.info(\"Création du dataset NER...\")\n",
        "        valid_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        # Limiter le dataset pour les tests\n",
        "        df_sample = df.head(5000) if len(df) > 5000 else df\n",
        "\n",
        "        for idx, row in df_sample.iterrows():\n",
        "            if idx % 500 == 0:\n",
        "                logger.info(f\"Traitement ligne {idx}/{len(df_sample)}\")\n",
        "\n",
        "            text = self.clean_text(row['text'])\n",
        "            keywords_list = self.parse_keywords(row['keywords'])\n",
        "\n",
        "            if not text or not keywords_list:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Tokeniser le texte\n",
        "            tokens = self.tokenize_sentence(text)\n",
        "            if len(tokens) < 3 or len(tokens) > self.config.max_sequence_length:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Créer les labels avec fuzzy matching\n",
        "            token_labels = self.find_keyword_matches(tokens, keywords_list)\n",
        "\n",
        "            # Vérifier qu'il y a au moins un mot-clé identifié\n",
        "            if sum(token_labels) == 0:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            sentences.append(tokens)\n",
        "            labels.append(token_labels)\n",
        "            valid_count += 1\n",
        "\n",
        "        logger.info(f\"Dataset créé: {valid_count} phrases valides, {skipped_count} ignorées\")\n",
        "\n",
        "        # Statistiques\n",
        "        total_keywords = sum(sum(label_seq) for label_seq in labels)\n",
        "        total_tokens = sum(len(label_seq) for label_seq in labels)\n",
        "        keyword_ratio = total_keywords / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "        logger.info(f\"Statistiques: {total_keywords} mots-clés sur {total_tokens} mots ({keyword_ratio:.2%})\")\n",
        "\n",
        "        return sentences, labels\n",
        "\n",
        "    def prepare_ner_data(self, sentences: List, labels: List) -> Tuple:\n",
        "        \"\"\"Préparation des données avec division simple\"\"\"\n",
        "        logger.info(\"Préparation des données...\")\n",
        "\n",
        "        # Filtrer les séquences valides\n",
        "        valid_data = []\n",
        "        for sent, lab in zip(sentences, labels):\n",
        "            if len(sent) == len(lab) and len(sent) > 0:\n",
        "                keyword_ratio = sum(lab) / len(lab)\n",
        "                # Garder les phrases avec un ratio raisonnable de mots-clés\n",
        "                if 0.01 <= keyword_ratio <= 0.5:\n",
        "                    valid_data.append((sent, lab))\n",
        "\n",
        "        if len(valid_data) < 100:\n",
        "            raise ValueError(\"Pas assez de données valides pour l'entraînement\")\n",
        "\n",
        "        sentences, labels = zip(*valid_data)\n",
        "        sentences, labels = list(sentences), list(labels)\n",
        "\n",
        "        # Division simple\n",
        "        train_size = int(0.7 * len(sentences))\n",
        "        val_size = int(0.15 * len(sentences))\n",
        "\n",
        "        sentences_train = sentences[:train_size]\n",
        "        labels_train = labels[:train_size]\n",
        "\n",
        "        sentences_val = sentences[train_size:train_size + val_size]\n",
        "        labels_val = labels[train_size:train_size + val_size]\n",
        "\n",
        "        sentences_test = sentences[train_size + val_size:]\n",
        "        labels_test = labels[train_size + val_size:]\n",
        "\n",
        "        logger.info(f\"Division: Train={len(sentences_train)}, Val={len(sentences_val)}, Test={len(sentences_test)}\")\n",
        "        return sentences_train, sentences_val, sentences_test, labels_train, labels_val, labels_test\n",
        "\n",
        "    def create_ner_tokenizer(self, sentences_train: List, sentences_val: List):\n",
        "        \"\"\"Création du tokenizer NER\"\"\"\n",
        "        all_words = []\n",
        "        for sentence in sentences_train + sentences_val:\n",
        "            all_words.extend(sentence)\n",
        "\n",
        "        word_freq = Counter(all_words)\n",
        "        # Garder les mots qui apparaissent au moins 2 fois\n",
        "        filtered_words = [word for word, freq in word_freq.items() if freq >= 2]\n",
        "\n",
        "        self.ner_tokenizer = Tokenizer(\n",
        "            num_words=self.config.vocab_size,\n",
        "            oov_token=self.config.oov_token,\n",
        "            filters='',\n",
        "            lower=True\n",
        "        )\n",
        "\n",
        "        filtered_sentences = [\" \".join(sentence) for sentence in sentences_train + sentences_val]\n",
        "        self.ner_tokenizer.fit_on_texts(filtered_sentences)\n",
        "\n",
        "        logger.info(f\"Tokenizer créé: vocab_size={len(self.ner_tokenizer.word_index)}\")\n",
        "        logger.info(f\"Mots les plus fréquents: {list(self.ner_tokenizer.word_index.items())[:10]}\")\n",
        "\n",
        "    def tokenize_and_pad_ner(self, sentences: List, labels: List, maxlen: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Tokenisation et padding pour NER\"\"\"\n",
        "        sequences = []\n",
        "        valid_labels = []\n",
        "\n",
        "        for sentence, label_seq in zip(sentences, labels):\n",
        "            # Tokeniser\n",
        "            seq = self.ner_tokenizer.texts_to_sequences([\" \".join(sentence)])[0]\n",
        "\n",
        "            if len(seq) > 0:  # Garder seulement les séquences non vides\n",
        "                sequences.append(seq)\n",
        "                valid_labels.append(label_seq)\n",
        "\n",
        "        X = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "        y = []\n",
        "        for label_seq in valid_labels:\n",
        "            # Tronquer ou padding des labels\n",
        "            if len(label_seq) > maxlen:\n",
        "                padded_labels = label_seq[:maxlen]\n",
        "            else:\n",
        "                padded_labels = label_seq + [0] * (maxlen - len(label_seq))\n",
        "            y.append(padded_labels)\n",
        "\n",
        "        y = np.array(y).reshape(-1, maxlen, 1)\n",
        "\n",
        "        logger.info(f\"Données tokenisées: X.shape={X.shape}, y.shape={y.shape}\")\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    def compute_class_weights(self, labels_train: List) -> Dict:\n",
        "        \"\"\"Calcul des poids de classe\"\"\"\n",
        "        flat_labels = [label for label_seq in labels_train for label in label_seq]\n",
        "        unique_labels = np.unique(flat_labels)\n",
        "\n",
        "        # Ensure both 0 and 1 are present\n",
        "        all_classes = np.array([0, 1])\n",
        "        present_labels = np.union1d(unique_labels, all_classes)\n",
        "\n",
        "\n",
        "        class_weights = compute_class_weight('balanced', classes=present_labels, y=flat_labels)\n",
        "\n",
        "        # Limiter les poids extrêmes\n",
        "        class_weights = np.clip(class_weights, 0.5, 5.0)\n",
        "\n",
        "        # Ensure weights are a dictionary with keys 0 and 1\n",
        "        self.class_weights = dict(zip(present_labels, class_weights))\n",
        "        logger.info(f\"Poids de classe calculés: {self.class_weights}\")\n",
        "        return self.class_weights\n",
        "\n",
        "    def evaluate_ner_model(self, model, X_test, y_test, tokenizer) -> Dict:\n",
        "        \"\"\"Évaluation complète du modèle NER\"\"\"\n",
        "        predictions = model.predict(X_test, verbose=0)\n",
        "        y_pred = (predictions > 0.5).astype(int)\n",
        "\n",
        "        y_true_flat = []\n",
        "        y_pred_flat = []\n",
        "\n",
        "        for i in range(len(y_test)):\n",
        "            # Prendre seulement les tokens non-paddés\n",
        "            seq_len = np.sum(X_test[i] != 0)\n",
        "            y_true_flat.extend(y_test[i][:seq_len, 0])\n",
        "            y_pred_flat.extend(y_pred[i][:seq_len, 0])\n",
        "\n",
        "        # Éviter les erreurs avec des arrays vides\n",
        "        if len(y_true_flat) == 0 or len(set(y_true_flat)) < 2:\n",
        "            logger.warning(\"Pas assez de données pour l'évaluation\")\n",
        "            return {\n",
        "                'precision': 0.0,\n",
        "                'recall': 0.0,\n",
        "                'f1_score': 0.0,\n",
        "                'support': len(y_true_flat)\n",
        "            }\n",
        "\n",
        "        precision = precision_score(y_true_flat, y_pred_flat, average='binary', zero_division=0)\n",
        "        recall = recall_score(y_true_flat, y_pred_flat, average='binary', zero_division=0)\n",
        "        f1 = f1_score(y_true_flat, y_pred_flat, average='binary', zero_division=0)\n",
        "\n",
        "        metrics = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'support': len(y_true_flat)\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Métriques finales - Précision: {precision:.3f}, Rappel: {recall:.3f}, F1: {f1:.3f}\")\n",
        "        return metrics\n",
        "\n",
        "    def extract_keywords_from_text(self, model, text: str, tokenizer, maxlen: int) -> List[str]:\n",
        "        \"\"\"Extraction des mots-clés d'un texte donné\"\"\"\n",
        "        if not text.strip():\n",
        "            return []\n",
        "\n",
        "        # Nettoyer et tokeniser le texte\n",
        "        clean_text = self.clean_text(text)\n",
        "        tokens = self.tokenize_sentence(clean_text)\n",
        "        if not tokens:\n",
        "            return []\n",
        "\n",
        "        # Conversion en séquence\n",
        "        sequence = tokenizer.texts_to_sequences([\" \".join(tokens)])[0]\n",
        "        if not sequence:\n",
        "            return []\n",
        "\n",
        "        # Padding\n",
        "        X = pad_sequences([sequence], maxlen=maxlen, padding='post')\n",
        "\n",
        "        # Prédiction\n",
        "        predictions = model.predict(X, verbose=0)\n",
        "        y_pred = (predictions > 0.3).astype(int)  # Seuil plus bas\n",
        "\n",
        "        # Extraction des mots-clés\n",
        "        keywords = []\n",
        "        for i, (token, pred) in enumerate(zip(tokens, y_pred[0])):\n",
        "            if i < len(tokens) and i < len(y_pred[0]) and pred[0] == 1:\n",
        "                keywords.append(token)\n",
        "\n",
        "        return keywords\n",
        "\n",
        "# Custom Binary Crossentropy Loss with Class Weights\n",
        "def weighted_binary_crossentropy(class_weights):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true_flat = tf.cast(tf.reshape(y_true, [-1]), tf.float32)\n",
        "        y_pred_flat = tf.reshape(y_pred, [-1])\n",
        "\n",
        "        # Calculer les poids\n",
        "        weight_0 = class_weights.get(0, 1.0)\n",
        "        weight_1 = class_weights.get(1, 1.0)\n",
        "\n",
        "        # Ensure weights tensor is float32\n",
        "        weights = tf.where(tf.equal(y_true_flat, 1), tf.constant(weight_1, dtype=tf.float32), tf.constant(weight_0, dtype=tf.float32))\n",
        "\n",
        "        # Binary crossentropy\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true_flat, y_pred_flat)\n",
        "        weighted_bce = bce * weights\n",
        "\n",
        "        return tf.reduce_mean(weighted_bce)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Custom F1 Score Metric\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.precision.reset_state()\n",
        "        self.recall.reset_state()\n",
        "\n",
        "class KeywordExtractionModel:\n",
        "    \"\"\"Modèle simplifié pour extraction de mots-clés\"\"\"\n",
        "    def __init__(self, config: Config, vocab_size: int):\n",
        "        self.config = config\n",
        "        self.vocab_size = vocab_size\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self, maxlen: int, class_weights: Dict) -> Model:\n",
        "        \"\"\"Construction du modèle simplifié\"\"\"\n",
        "        inputs = Input(shape=(maxlen,), name='input_tokens')\n",
        "\n",
        "        # Embedding\n",
        "        embedding = Embedding(\n",
        "            input_dim=self.vocab_size,\n",
        "            output_dim=self.config.embedding_dim,\n",
        "            mask_zero=True,\n",
        "            name='embedding'\n",
        "        )(inputs)\n",
        "        x = Dropout(self.config.dropout_rate)(embedding)\n",
        "\n",
        "        # Couches convolutionnelles simplifiées\n",
        "        conv_outputs = []\n",
        "        for i, (filters, kernel_size) in enumerate(zip(self.config.filters, self.config.kernel_sizes)):\n",
        "            conv = Conv1D(\n",
        "                filters, kernel_size,\n",
        "                activation='relu',\n",
        "                padding='same',\n",
        "                name=f'conv_{i+1}'\n",
        "            )(x)\n",
        "            conv = Dropout(self.config.dropout_rate)(conv)\n",
        "            conv_outputs.append(conv)\n",
        "\n",
        "        # Concaténation\n",
        "        if len(conv_outputs) > 1:\n",
        "            x = Concatenate(name='concat_conv')(conv_outputs)\n",
        "        else:\n",
        "            x = conv_outputs[0]\n",
        "\n",
        "        # LSTM bidirectionnel\n",
        "        if self.config.use_bidirectional:\n",
        "            x = Bidirectional(\n",
        "                LSTM(64, return_sequences=True, dropout=self.config.dropout_rate),\n",
        "                name='bilstm'\n",
        "            )(x)\n",
        "\n",
        "        # Couche dense finale\n",
        "        x = TimeDistributed(\n",
        "            Dense(32, activation='relu'),\n",
        "            name='dense_1'\n",
        "        )(x)\n",
        "        x = Dropout(self.config.dropout_rate)(x)\n",
        "\n",
        "        # Sortie\n",
        "        outputs = TimeDistributed(Dense(1, activation='sigmoid'), name='keyword_output')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs, name='Keyword_Extraction_Model')\n",
        "\n",
        "        # Optimiseur\n",
        "        optimizer = AdamW(\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            clipnorm=self.config.gradient_clip_norm\n",
        "        )\n",
        "\n",
        "        # Loss function\n",
        "        custom_loss = weighted_binary_crossentropy(class_weights)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=custom_loss,\n",
        "            metrics=['accuracy', 'precision', 'recall', F1Score()]\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Modèle construit - Paramètres: {model.count_params():,}\")\n",
        "        return model\n",
        "\n",
        "    def create_callbacks(self, timestamp: str) -> List:\n",
        "        \"\"\"Callbacks pour l'entraînement\"\"\"\n",
        "        return [\n",
        "            EarlyStopping(\n",
        "                monitor='val_f1_score',\n",
        "                patience=self.config.patience_early_stopping,\n",
        "                restore_best_weights=True,\n",
        "                mode='max',\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                filepath=self.config.model_save_path / f\"best_keyword_model_{timestamp}.keras\",\n",
        "                monitor='val_f1_score',\n",
        "                save_best_only=True,\n",
        "                mode='max',\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=self.config.lr_factor,\n",
        "                patience=self.config.patience_lr_reduce,\n",
        "                min_lr=self.config.min_lr,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Gestionnaire d'entraînement du modèle d'extraction de mots-clés\"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    def train_keyword_model(self, data_processor: DataProcessor) -> Tuple[Model, Dict]:\n",
        "        \"\"\"Entraînement complet du modèle d'extraction de mots-clés\"\"\"\n",
        "        logger.info(\"=== DÉBUT ENTRAÎNEMENT MODÈLE EXTRACTION MOTS-CLÉS ===\")\n",
        "\n",
        "        # Chargement des données Parquet\n",
        "        df = data_processor.load_parquet_data()\n",
        "\n",
        "        # Création du dataset NER\n",
        "        sentences, labels = data_processor.create_ner_dataset(df)\n",
        "\n",
        "        if len(sentences) == 0:\n",
        "            raise ValueError(\"Aucune phrase valide trouvée dans les données\")\n",
        "\n",
        "        # Préparation des données\n",
        "        sentences_train, sentences_val, sentences_test, labels_train, labels_val, labels_test = \\\n",
        "            data_processor.prepare_ner_data(sentences, labels)\n",
        "\n",
        "        # Création du tokenizer\n",
        "        data_processor.create_ner_tokenizer(sentences_train, sentences_val)\n",
        "\n",
        "        # Calcul de la longueur maximale\n",
        "        all_sentences = sentences_train + sentences_val\n",
        "        max_len_found = max(len(s) for s in all_sentences) if all_sentences else 50\n",
        "        maxlen = min(max_len_found, self.config.max_sequence_length)\n",
        "\n",
        "        logger.info(f\"Longueur max des séquences: {maxlen}\")\n",
        "\n",
        "        # Tokenisation et padding\n",
        "        X_train, y_train = data_processor.tokenize_and_pad_ner(sentences_train, labels_train, maxlen)\n",
        "        X_val, y_val = data_processor.tokenize_and_pad_ner(sentences_val, labels_val, maxlen)\n",
        "        X_test, y_test = data_processor.tokenize_and_pad_ner(sentences_test, labels_test, maxlen)\n",
        "\n",
        "        # Vérification des données\n",
        "        if X_train.shape[0] == 0:\n",
        "            raise ValueError(\"Aucune donnée d'entraînement valide\")\n",
        "\n",
        "        # Calcul des poids de classe\n",
        "        class_weights = data_processor.compute_class_weights(labels_train)\n",
        "\n",
        "        # Construction du modèle\n",
        "        model_builder = KeywordExtractionModel(self.config, len(data_processor.ner_tokenizer.word_index) + 1)\n",
        "        model = model_builder.build_model(maxlen, class_weights)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = model_builder.create_callbacks(self.timestamp)\n",
        "\n",
        "        # Entraînement\n",
        "        logger.info(\"Début de l'entraînement...\")\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=self.config.epochs_ner,\n",
        "            batch_size=self.config.batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Évaluation\n",
        "        metrics = data_processor.evaluate_ner_model(model, X_test, y_test, data_processor.ner_tokenizer)\n",
        "\n",
        "        # Test d'extraction de mots-clés\n",
        "        test_texts = [\n",
        "            \"This is an example text to test automatic keyword extraction from important content.\",\n",
        "            \"Machine learning models can extract important keywords from text documents.\",\n",
        "            \"Natural language processing helps computers understand human language.\"\n",
        "        ]\n",
        "\n",
        "        for test_text in test_texts:\n",
        "            extracted_keywords = data_processor.extract_keywords_from_text(\n",
        "                model, test_text, data_processor.ner_tokenizer, maxlen\n",
        "            )\n",
        "            logger.info(f\"Test - Texte: '{test_text}'\")\n",
        "            logger.info(f\"Mots-clés extraits: {extracted_keywords}\")\n",
        "\n",
        "        # Sauvegarde\n",
        "        model_path = self.config.model_save_path / f\"keyword_extraction_model_{self.timestamp}.keras\"\n",
        "        model.save(model_path)\n",
        "\n",
        "        tokenizer_path = self.config.model_save_path / f\"keyword_tokenizer_{self.timestamp}.pkl\"\n",
        "        with open(tokenizer_path, 'wb') as f:\n",
        "            pickle.dump(data_processor.ner_tokenizer, f)\n",
        "\n",
        "        # Sauvegarde des résultats\n",
        "        results = {\n",
        "            'model_path': str(model_path),\n",
        "            'tokenizer_path': str(tokenizer_path),\n",
        "            'metrics': metrics,\n",
        "            'history': {k: [float(v) for v in vals] for k, vals in history.history.items()},\n",
        "            'config': {k: str(v) if isinstance(v, Path) else v for k, v in self.config.__dict__.items()},\n",
        "            'maxlen': maxlen,\n",
        "            'vocab_size': len(data_processor.ner_tokenizer.word_index),\n",
        "            'test_extractions': [\n",
        "                {\n",
        "                    'text': text,\n",
        "                    'keywords': data_processor.extract_keywords_from_text(\n",
        "                        model, text, data_processor.ner_tokenizer, maxlen\n",
        "                    )\n",
        "                }\n",
        "                for text in test_texts\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        results_path = self.config.results_path / f\"keyword_extraction_results_{self.timestamp}.json\"\n",
        "        with open(results_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"=== ENTRAÎNEMENT TERMINÉ - F1 Score: {metrics['f1_score']:.3f} ===\")\n",
        "        return model, results\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fonction principale d'exécution\"\"\"\n",
        "    try:\n",
        "        logger.info(\"=== DÉBUT DU PIPELINE D'EXTRACTION DE MOTS-CLÉS ===\")\n",
        "\n",
        "        # Configuration\n",
        "        config = Config()\n",
        "        logger.info(f\"Configuration chargée\")\n",
        "\n",
        "        # Initialisation\n",
        "        data_processor = DataProcessor(config)\n",
        "        trainer = ModelTrainer(config)\n",
        "\n",
        "        # Entraînement du modèle d'extraction de mots-clés\n",
        "        logger.info(\"Lancement de l'entraînement du modèle d'extraction de mots-clés...\")\n",
        "        keyword_model, keyword_results = trainer.train_keyword_model(data_processor)\n",
        "\n",
        "        # Résumé final\n",
        "        logger.info(\"=== ENTRAÎNEMENT TERMINÉ AVEC SUCCÈS ===\")\n",
        "        logger.info(f\"Modèle d'extraction de mots-clés - F1 Score: {keyword_results['metrics']['f1_score']:.3f}\")\n",
        "        logger.info(f\"Modèle sauvegardé dans: {config.model_save_path}\")\n",
        "        logger.info(f\"Résultats sauvegardés dans: {config.results_path}\")\n",
        "\n",
        "        return {\n",
        "            'keyword_model': keyword_model,\n",
        "            'keyword_results': keyword_results,\n",
        "            'data_processor': data_processor\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Erreur lors de l'exécution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Fonction utilitaire pour charger un modèle sauvegardé\n",
        "def load_trained_model(model_path: str, tokenizer_path: str, config: Config = None):\n",
        "    \"\"\"Charge un modèle et tokenizer sauvegardés\"\"\"\n",
        "    if config is None:\n",
        "        config = Config()\n",
        "\n",
        "    # Charger le modèle\n",
        "    model = load_model(model_path, custom_objects={\n",
        "        'F1Score': F1Score,\n",
        "        'weighted_binary_crossentropy': weighted_binary_crossentropy\n",
        "    })\n",
        "\n",
        "    # Charger le tokenizer\n",
        "    with open(tokenizer_path, 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "\n",
        "    # Créer le data processor\n",
        "    data_processor = DataProcessor(config)\n",
        "    data_processor.ner_tokenizer = tokenizer\n",
        "\n",
        "    return model, data_processor\n",
        "\n",
        "# Fonction utilitaire pour extraire des mots-clés d'un nouveau texte\n",
        "def extract_keywords(text: str, model_path: str, tokenizer_path: str, maxlen: int = 128):\n",
        "    \"\"\"Extrait les mots-clés d'un texte avec un modèle pré-entraîné\"\"\"\n",
        "    config = Config()\n",
        "    model, data_processor = load_trained_model(model_path, tokenizer_path, config)\n",
        "\n",
        "    keywords = data_processor.extract_keywords_from_text(\n",
        "        model, text, data_processor.ner_tokenizer, maxlen\n",
        "    )\n",
        "\n",
        "    return keywords\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration de l'environnement\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "    # Gestion de la mémoire GPU\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        except RuntimeError as e:\n",
        "            logger.warning(f\"Erreur configuration GPU: {e}\")\n",
        "\n",
        "    # Exécution\n",
        "    results = main()"
      ]
    }
  ]
}