{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af4586b",
   "metadata": {},
   "source": [
    "# Mod√®le DL RNN et LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d39339",
   "metadata": {},
   "source": [
    "### Modele synth√®se (Uniquement BART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84074326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Texte original :\n",
      "\n",
      "Artificial intelligence is transforming the way we work and live. From smart assistants to automated medical diagnostics, AI technologies are becoming increasingly integrated into our daily lives. However, this rapid progress also raises important questions about privacy, job displacement, and the ethical use of data.\n",
      "As AI continues to evolve, it is crucial for society to address these challenges and ensure that the benefits of AI are shared equitably. Policymakers, technologists, and the public must work together to create a framework that promotes innovation while safeguarding individual rights and societal values.\n",
      "The future of AI holds great promise, but it is essential to navigate the complexities it brings. By fostering collaboration and open dialogue, we can harness the power of AI to improve our world while minimizing its risks.\n",
      "\n",
      "\n",
      "üìù R√©sum√© g√©n√©r√© :\n",
      "Artificial intelligence is transforming the way we work and live. From smart assistants to automated medical diagnostics, AI technologies are becoming increasingly integrated into our daily lives. However, this rapid progress also raises important questions about privacy, job displacement,\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# 1. Chargement du mod√®le pr√©-entra√Æn√© BART (base ou large)\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 2. Texte d'entr√©e\n",
    "texte = \"\"\"\n",
    "Artificial intelligence is transforming the way we work and live. From smart assistants to automated medical diagnostics, AI technologies are becoming increasingly integrated into our daily lives. However, this rapid progress also raises important questions about privacy, job displacement, and the ethical use of data.\n",
    "As AI continues to evolve, it is crucial for society to address these challenges and ensure that the benefits of AI are shared equitably. Policymakers, technologists, and the public must work together to create a framework that promotes innovation while safeguarding individual rights and societal values.\n",
    "The future of AI holds great promise, but it is essential to navigate the complexities it brings. By fostering collaboration and open dialogue, we can harness the power of AI to improve our world while minimizing its risks.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Pr√©traitement : tokenisation\n",
    "inputs = tokenizer(texte, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# 4. Encodage du texte (embedding + encoder)\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.model.encoder(**inputs)\n",
    "    hidden_states = encoder_outputs.last_hidden_state  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "# 5. D√©codage\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=50,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 6. Post-traitement : g√©n√©ration du r√©sum√© \"unembedding\"\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"üìÑ Texte original :\")\n",
    "print(texte)\n",
    "print(\"\\nüìù R√©sum√© g√©n√©r√© :\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048dc8f",
   "metadata": {},
   "source": [
    "Pour la classification regarder DistilBERT, RoBERTa, DeBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab97915a",
   "metadata": {},
   "source": [
    "## Mod√®le maison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8f0ae",
   "metadata": {},
   "source": [
    "Importation des donn√©es d'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f53e76",
   "metadata": {},
   "source": [
    "Dataset :\n",
    "- CNN/Dailymail\n",
    "- Wikitext : https://huggingface.co/datasets/Salesforce/wikitext/viewer/wikitext-103-raw-v1/train?p=1&views%5B%5D=wikitext_103_raw_v1_train&row=135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba232f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # affiche tous les logs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # forcer usage GPU 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc08b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPUs d√©tect√©s : []\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4622351559110675773\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs d√©tect√©s :\", gpus)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f2d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported ¬£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest ¬ª . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'highlights': \"Harry Potter star Daniel Radcliffe gets ¬£20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\", 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}\n",
      "{'train': (287113, 3), 'validation': (13368, 3), 'test': (11490, 3)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(dataset[\"train\"][0])\n",
    "print(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a31d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# df_train = dataset[\"train\"]\n",
    "df_train = dataset[\"train\"]#.shuffle(seed=42).select(range(5000)) √† tester pour faciliter entrainnement du mod√®le\n",
    "x_train = df_train[\"article\"]\n",
    "y_train = df_train[\"highlights\"]\n",
    "\n",
    "df_test = dataset[\"test\"]\n",
    "x_test = df_test[\"article\"]\n",
    "y_test = df_test[\"highlights\"]\n",
    "\n",
    "df_val = dataset[\"validation\"]\n",
    "x_val = df_val[\"article\"]\n",
    "y_val = df_val[\"highlights\"]\n",
    "\n",
    "mini_x_train = x_train[:1000]\n",
    "mini_y_train = y_train[:1000]\n",
    "\n",
    "mini_x_test = x_test[:100]\n",
    "mini_y_test = y_test[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9784090d",
   "metadata": {},
   "source": [
    "## Mod√®le pour synth√®se RNN et LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd2132",
   "metadata": {},
   "source": [
    "### Mod√®le synth√®se from scratch Seq2Seq (Encoder‚ÄìDecoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613dc00c",
   "metadata": {},
   "source": [
    "#### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e61435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 216, 305, 6588, 1494, 6133, 384, 1663, 13473, 6916, 1020, 3, 4, 287, 5379, 217, 2102, 134, 217, 3842, 19, 13, 3094, 392, 10, 311, 26, 13, 2783, 2, 261, 1349, 2502, 4, 4181, 10, 61, 1663, 13473, 19, 1494, 6133, 7, 1494, 6133, 5, 2, 603, 6, 2, 4892, 3, 2, 6514, 6, 10081, 38521, 124, 2, 105, 2, 234, 1569, 93, 13, 23, 75, 656, 3, 84545, 16, 1259, 211, 10, 1420, 1096, 1572, 5, 2159, 1761, 31, 226, 725, 3, 24, 44, 6, 139, 59, 33, 19, 697, 19, 30, 826, 392, 2846, 1107, 807, 4, 1529, 1144, 231, 1435, 53, 307, 798, 13, 73, 29, 1008, 15644, 343, 35, 203, 31, 226, 165, 3178, 24, 1308, 10797, 2, 396, 31, 89, 2570, 27, 396, 11, 664, 52, 185, 1751, 1888, 5, 19496, 5, 13735, 18, 392, 13473, 41, 24, 328, 3, 10928, 7, 4, 6026, 1107, 4, 1572, 7, 4, 3075, 53, 155, 2, 3513, 591, 13059, 177, 2387, 686, 232, 1535, 1563, 16, 252, 44, 1306, 10, 2, 314, 1438, 325, 7605, 1016, 6, 113, 4924, 630, 16, 5522, 1706, 27, 161, 13067, 16, 1936, 5, 10150, 32, 75, 980, 10, 16, 656, 3178, 2575, 22, 88, 1925, 6, 308, 13, 15, 7, 29, 760, 4070, 2209, 6, 54, 41, 24, 1912, 52, 17, 100512, 6444, 21, 2, 72, 182, 6133, 2507, 22, 38, 424, 7, 4, 1145, 1749, 48, 13, 23, 34, 38, 328, 3, 1801, 340, 16, 1047, 3209, 5, 15626, 2, 1569, 93, 13, 12, 1784, 16, 949, 6987, 10, 2, 624, 59, 27, 352, 393, 3, 132, 105486, 384, 1118, 106, 2, 16607, 37, 13, 73, 1402, 65, 203, 26, 31, 613, 125, 492, 34, 3, 157, 11, 138, 97, 17, 57, 24, 240, 1461, 9, 77, 16, 721, 10516, 19, 2, 614, 16112, 7, 1494, 6133, 5, 2, 603, 6, 2, 4892, 12, 1843, 1350, 10, 187, 2021, 6, 2, 3143, 5, 13, 41, 35164, 2, 657, 7, 2, 65, 58, 2507, 524, 31, 1011, 378, 25, 1526, 6, 31135, 721, 2346, 64, 12, 126, 1625, 6133, 275, 2, 21822, 23, 2463, 4, 583, 1306, 173, 83, 614, 1822, 52, 1966, 44111, 33428, 5, 16, 296, 507, 9, 851, 193, 35, 46, 13, 41, 56, 1149, 7, 513, 1232, 29, 1008, 591, 52, 145, 1232, 33, 1949, 29, 12620, 343, 35, 46, 13, 107, 16, 960, 2424, 646, 4, 6686, 1670, 7, 1255, 94230, 111204, 1267, 13, 12, 19576, 9, 131, 2398, 390, 5530, 82, 11, 584, 4496, 29, 2280, 31, 70, 165, 408, 149, 3, 24, 51, 1925, 6, 1815, 255, 13, 73, 6588, 953, 571, 3, 4, 504, 6394, 1034, 6588, 60, 522, 4720, 35, 2104, 120, 34, 24, 257, 2992, 24441, 53, 30758, 43]\n",
      "Index des tokens sp√©ciaux :\n",
      "sos index: 42\n",
      "eos index: 43\n",
      "unk index: None\n",
      "Taille vocabulaire: 813596\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# --- 1. Tokenisation \n",
    "num_word_token = 500000\n",
    "tokenizer = Tokenizer(num_words=num_word_token, oov_token=\"<unk>\")\n",
    "\n",
    "complet_text = x_train + y_train\n",
    "\n",
    "#Ajout de token sp√©ciaux pour le mod√®le (afin qu'il reconnaisse les s√©quences de d√©but et de fin et pas a la fin de chaque phrase car sinon on casse le rythme du mod√®le seq to seq)\n",
    "texts_for_fit = [\"<sos> \" + s + \" <eos>\" for s in complet_text]\n",
    "texts_for_fit_x = [\"<sos> \" + s + \" <eos>\" for s in x_train]\n",
    "texts_for_fit_y = [\"<sos> \" + s + \" <eos>\" for s in y_train]\n",
    "\n",
    "tokenizer.fit_on_texts(texts_for_fit)\n",
    "\n",
    "# --- 2. Conversion en s√©quences\n",
    "x_train_seq = tokenizer.texts_to_sequences(texts_for_fit_x)\n",
    "\n",
    "print(x_train_seq[0])\n",
    "# V√©rification que les tokens sp√©ciaux sont bien index√©s\n",
    "print(\"Index des tokens sp√©ciaux :\")\n",
    "print(\"sos index:\", tokenizer.word_index.get('sos'))\n",
    "print(\"eos index:\", tokenizer.word_index.get('eos'))\n",
    "print(\"unk index:\", tokenizer.word_index.get('unk')) #=> Pour les mots inconnus donc retourne None\n",
    "print(\"Taille vocabulaire:\", len(tokenizer.word_index))\n",
    "\n",
    "#Le tokenizer enl√®ve les caract√®re sp√©ciaux, comme les apostrophes, les tirets, etc. => Ce qui explique pourquoi on ne voit plus les <>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baaaac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de texte sup √† 250 tokens : 13704 ce qui repr√©sente 4.77% du dataset\n",
      "Nous allons partir sur un troncage √† 250 token pour facilit√© l'entrainnement du mod√®le\n"
     ]
    }
   ],
   "source": [
    "# Obtenir le nombre max de token d'une sequence sur tout le dataset\n",
    "max_len = max(len(seq) for seq in x_train_seq)\n",
    "print(max_len)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seq_lengths = [len(seq) for seq in x_train_seq]\n",
    "plt.hist(seq_lengths, bins=50)\n",
    "plt.xlabel('Longueur des s√©quences')\n",
    "plt.ylabel('Nombre de s√©quences')\n",
    "plt.show()\n",
    "\n",
    "len_token_sentence = 250\n",
    "\n",
    "seq_lengths = np.array(seq_lengths)\n",
    "nb_inf = np.sum(seq_lengths < len_token_sentence)\n",
    "part_nb_inf = round((nb_inf/len(seq_lengths))*100,2)\n",
    "print(f'Nombre de texte sup √† 250 tokens : {nb_inf} ce qui repr√©sente {part_nb_inf}% du dataset')\n",
    "print(\"Nous allons partir sur un troncage √† 250 token pour facilit√© l'entrainnement du mod√®le\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45e577",
   "metadata": {},
   "source": [
    "### Filtrage du dataset pour √©viter le troncage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ca0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13704\n",
      "249\n",
      "[[42, 627, 129, 1764, 156, 7723, 9771, 41, 1864, 19, 2201, 156, 4981, 395, 96, 156, 1664, 12, 108176, 9, 4, 3414, 37980, 295, 150, 400, 1910, 1445, 15, 339, 1664, 12, 1938, 3, 22, 2, 515, 2996, 474, 3, 137, 52, 194, 134, 194, 282, 18, 2, 1383, 6440, 18, 1374, 330, 3999, 1445, 15, 9143, 65, 37980, 8, 7, 540, 2167, 5, 75, 18071, 39, 109, 1445, 15, 2, 3329, 1230, 32, 4218, 4, 4944, 2996, 7, 52, 182, 74, 2, 2996, 41, 24, 11061, 20, 511, 1244, 79420, 5, 2553, 20, 4, 64342, 162, 21, 2, 237, 5119, 515, 618, 7, 24681, 3999, 1445, 15, 4, 37980, 12, 2, 115, 3856, 823, 9, 14001, 619, 41746, 619, 5, 60936, 495, 28211, 6, 2616, 11, 66, 370, 14174, 136, 3, 2, 12987, 3102, 495, 60936, 120, 24, 1340, 104, 2, 2996, 1445, 15, 11, 8, 2, 215, 45, 1664, 32, 61472, 81, 1505, 156, 1445, 454, 12, 5278, 5250, 9, 619, 11, 452, 7, 16, 14001, 5, 1631, 3, 16, 4857, 1445, 73, 1402, 13, 32, 4, 19339, 2178, 1938, 193, 339, 524, 1445, 886, 52, 9143, 2996, 5, 16, 207, 14001, 619, 2346, 2, 156, 805, 3, 3009, 2661, 3, 246, 2372, 1445, 15, 2, 286, 619, 1275, 8759, 11, 59, 266, 178, 649, 3982, 53, 2125, 1780, 475, 9485, 9, 1573, 6, 36432, 619, 18, 508, 535, 953, 571, 3, 4, 504, 43], [42, 1090, 2704, 673, 129, 4, 6688, 298, 194, 4103, 7201, 2, 1090, 2704, 320, 339, 18, 298, 1907, 4, 938, 20093, 405, 1907, 4, 938, 5088, 2, 119, 121, 11198, 1776, 287, 2, 6598, 99, 52, 194, 98, 1104, 266, 487, 15, 330, 17687, 4, 400, 9, 3225, 1250, 5, 726, 161, 2, 22260, 17501, 4, 6688, 298, 194, 4103, 12, 1197, 726, 48, 17, 93, 1598, 2350, 9337, 1052, 49, 32, 936, 4, 8360, 7, 895, 2515, 895, 6, 2252, 2209, 6, 128, 933, 2209, 6, 128, 1052, 11, 8, 287, 15, 7263, 376, 459753, 6, 2, 1090, 2704, 71, 17, 8, 5258, 7080, 524, 71, 3262, 1441, 895, 1044, 36, 2, 6598, 2346, 2, 6598, 8, 11008, 52, 58, 631, 548, 4960, 6, 7734, 18, 4, 5930, 6, 248, 288, 631, 2, 22260, 15, 7734, 12, 70, 548, 6, 1090, 2704, 300, 1090, 2704, 1981, 29, 7734, 71, 9454, 73, 129, 2, 6598, 208, 106, 14319, 18, 2903, 973, 2, 6204, 4864, 52, 535, 1820, 15, 129, 11179, 8724, 8689, 136, 3, 2, 22260, 6688, 298, 194, 23018, 27, 712, 11379, 5, 120, 858, 7091, 5, 2621, 5, 11977, 11335, 4010, 32864, 17131, 120, 510, 953, 571, 3, 4, 504, 43], [42, 14467, 3560, 6588, 12108, 1154, 4149, 183, 23, 38, 1633, 36, 4, 10086, 6761, 7, 5512, 9, 2, 944, 15, 2815, 8605, 251020, 336, 1735, 48, 12, 23198, 2, 183, 2030, 288468, 493, 5, 685, 86, 7884, 32, 820, 3, 84546, 3536, 1781, 7, 12108, 32440, 20880, 154, 3, 3090, 9, 2, 346553, 4536, 754, 48, 2825, 4, 3319, 6, 1809, 2620, 9, 4277, 4, 319, 21, 5854, 35059, 10, 311, 15, 288468, 236, 2846, 5, 13, 8, 291, 3, 22, 6761, 60, 2373, 3, 8022, 61, 20, 2, 8818, 515, 162, 5, 2, 18927, 171, 16, 1214, 7884, 750, 15, 5854, 35059, 48, 12, 1359, 1802, 20, 2, 2815, 1644, 7307, 32, 38, 507, 3, 416, 10, 394, 7, 2, 183, 2, 23719, 7884, 27, 13801, 44, 20, 44, 264, 4, 1391, 11962, 2, 2348, 261, 12, 4, 301, 5834, 7, 4, 210, 101, 115, 59, 417, 7, 2389, 3356, 5, 1983, 279, 21, 12108, 1128, 3611, 2, 1391, 8, 3, 110, 242, 217, 70012, 52, 2061, 98, 7, 1259, 4, 145, 3656, 1204, 11888, 5, 159, 866, 98, 70012, 52, 248, 5154, 3, 1107, 1890, 2, 1391, 79, 56, 22, 474, 3, 370, 29, 6334, 2159, 7977, 9341, 3452, 2, 4536, 754, 48, 363, 7, 1681, 1373, 178, 5065, 953, 571, 3, 4, 504, 6394, 1034, 6588, 60, 522, 4720, 35, 2104, 120, 34, 24, 257, 2992, 24441, 53, 30758, 43], [42, 216, 305, 768, 27, 1341, 10, 2, 2845, 6, 367, 2981, 607, 6, 6424, 1014, 263, 518, 14, 5461, 26, 2519, 5714, 23, 38, 1576, 50, 367, 2981, 3477, 50, 16, 4479, 5466, 104, 609, 9, 768, 10, 329, 618, 95, 2981, 716, 4, 1447, 26869, 104, 4670, 653, 653, 1597, 14, 6109, 5, 768, 567, 40861, 2866, 41, 155, 113, 13, 10205, 104, 609, 10, 329, 81, 350, 4, 456, 10, 16, 10929, 2981, 2471, 18, 37555, 50122, 1360, 14, 4, 258, 5466, 10, 36, 981, 10, 316, 367, 2981, 805, 3, 342, 48, 12, 125, 198, 82, 49, 254, 3, 1550, 9, 609, 5, 111, 49, 41, 1282, 14, 2, 515, 440, 5, 2200, 15, 2866, 2866, 23, 871, 11, 5714, 41, 2575, 1894, 2, 255, 50, 468, 19, 2, 1639, 1558, 3, 3587, 21, 16, 6919, 933, 1639, 531, 32866, 33, 1079, 4, 65, 945, 1391, 9, 768, 3, 5479, 5461, 50, 6, 65, 7727, 1014, 263, 23, 56, 38, 4232, 4, 1486, 933, 26, 13, 188, 177, 7, 609, 10, 329, 5, 12, 474, 3, 342, 953, 571, 3, 4, 504, 43], [42, 1422, 849, 129, 13238, 10, 311, 188, 2, 224898, 1070, 6, 350, 4, 1086, 1272, 9, 184, 3, 2438, 4, 135, 21, 17035, 24085, 989, 220, 21, 2, 907, 11, 17, 15, 672, 61, 2475, 5580, 8230, 1232, 35, 1165, 882, 413, 113, 71, 167, 2919, 3, 156883, 2, 882, 951, 13238, 2, 3750, 258, 8, 13700, 20, 2820, 42110, 26, 2, 827, 39, 3336, 6, 87, 5730, 136, 3, 4, 14431, 21, 13238, 2, 289, 71, 714, 420, 7, 9171, 849, 13238, 643, 592, 4891, 4569, 8144, 15, 2, 905, 22, 38, 10, 2, 2, 907, 9, 332, 74, 26, 1153, 22, 38, 1238, 3, 2416, 2, 3750, 2432, 53, 12943, 49, 22, 546, 60, 86, 717, 3, 2438, 5, 3, 815, 61, 3, 693, 26, 49, 27, 82, 3599, 11, 266, 2, 11593, 184, 35, 785, 9179, 79, 666, 3, 1522, 5, 2475, 840, 234, 142, 692, 3734, 1149, 3, 927, 21, 232, 3, 354, 3716, 8144, 15, 13, 15, 64, 12, 125, 198, 1125, 3, 353, 11, 13, 6411, 2, 105, 7, 603, 3, 2475, 840, 5, 10439, 2714, 142, 13238, 23, 1804, 2, 989, 39, 220, 7, 4713, 5, 9521, 2, 456, 3, 127, 204, 35, 3750, 684, 8, 34, 44, 48, 8, 220, 11462, 15, 18855, 204666, 1786, 536, 6, 41038, 4323, 7, 541, 10311, 1735, 2, 12057, 890, 5, 51, 435, 66, 24, 209, 528, 18, 41038, 1807, 512, 953, 571, 3, 4, 504, 43], [42, 129, 2, 4246, 363, 19, 4, 887, 1501, 10, 3581, 441, 7, 216, 1141, 7, 30598, 20, 367, 2064, 205, 2, 4246, 12, 29, 1692, 14, 7128, 1280, 2, 314, 14, 6964, 5, 440, 2254, 13117, 4047, 4088, 577, 59, 60, 4088, 577, 3199, 444, 27, 2945, 33, 207, 817, 367, 2064, 440, 2254, 22049, 10355, 6964, 29, 528, 5, 11411, 467, 367, 2064, 2202, 4, 2202, 703, 306, 459801, 94, 2013, 3896, 5, 4, 2319, 247, 1175, 5847, 2, 214, 2216, 6, 6288, 420, 10, 4, 4930, 5042, 6, 40, 1611, 4562, 2, 6288, 9, 1290, 8, 392, 4062, 3, 585, 362, 436, 48, 8, 4814, 50, 9, 247, 2634, 1265, 1098, 3757, 38668, 2016, 18, 2, 239, 6, 478, 1034, 16, 7022, 12, 2830, 36436, 149082, 4167, 9, 439, 817, 1290, 3, 439, 817, 1034, 8, 134, 4163, 217, 48, 957, 2, 4246, 6288, 6, 2893, 98, 367, 28263, 25710, 3873, 9, 2, 65, 1012, 46, 8, 312, 298, 1022, 953, 571, 3, 4, 504, 43], [42, 129, 2, 306, 8, 3074, 7, 6364, 20, 450, 3040, 1106, 3020, 6554, 4464, 142216, 38815, 1552, 5727, 9328, 459802, 12944, 11468, 1552, 288479, 5, 6248, 295, 44, 6, 96182, 72, 1433, 8, 459803, 2174, 7, 5568, 48, 12, 686, 2, 1191, 3436, 420, 2240, 1234, 428, 9, 2, 3500, 1106, 205, 96182, 7668, 10291, 1366, 4047, 288, 577, 175, 269, 16235, 5, 7668, 4915, 9, 121779, 5, 1315, 6454, 51, 68, 5780, 14650, 2000, 5809, 2580, 22, 8466, 96182, 4196, 121779, 16235, 32354, 12, 372, 2, 346, 6, 2, 121, 1652, 866, 5890, 3842, 866, 5, 4, 1391, 6, 2, 119, 121, 440, 6, 39899, 643, 6, 39899, 1266, 1758, 2, 306, 23, 38, 2678, 372, 66296, 577, 238, 1059, 3, 148, 9, 7, 795, 9, 685, 74, 7, 4, 1562, 5, 2, 11527, 814, 6, 115, 10638, 1059, 96182, 1611, 3873, 9, 1290, 8, 405, 2556, 1022, 14, 4, 2529, 2057, 6, 194, 2089, 1022, 953, 571, 3, 4, 504, 43], [42, 1422, 849, 5504, 19341, 3222, 1639, 16703, 23, 38, 732, 668, 623, 6, 2, 46, 6420, 849, 10646, 11519, 11784, 1758, 16703, 23, 324, 446, 60, 6, 2, 12522, 576, 9274, 16, 1121, 501, 58, 74, 36, 16, 1214, 24219, 10164, 21710, 446, 2, 1758, 2, 493, 46, 84, 16703, 8, 4, 576, 2671, 7, 5504, 19341, 17324, 1014, 263, 447, 2, 4775, 47, 8, 459, 4462, 5446, 4526, 4609, 2540, 14, 10164, 11336, 1639, 4854, 2769, 4602, 457, 35, 12, 125, 718, 9, 116, 17, 41748, 29, 6480, 46, 9, 116, 16703, 15, 117, 2, 228, 2348, 124, 5, 2, 100, 138, 3, 349, 307, 89, 35, 12, 3, 342, 9, 4, 162, 89, 5504, 2711, 117, 297, 3, 24, 177, 6, 4, 162, 11, 2718, 18, 493, 74, 84, 13, 23, 324, 489, 60, 2, 12522, 576, 9274, 12255, 5, 13488, 13, 8, 177, 6, 7402, 2167, 105, 323, 923, 1329, 589, 13, 8, 2094, 3, 70, 575, 443, 19, 4, 5086, 114, 2354, 6773, 13, 8, 228, 7570, 7, 65, 7727, 1014, 263, 1437, 2711, 3, 21794, 40, 1003, 3, 766, 7, 2, 1520, 398, 13, 489, 2, 1545, 1798, 1072, 7, 16, 72, 278, 18, 2711, 267, 1049, 21, 17325, 7647, 7100, 9, 543, 242, 217, 4, 5690, 11, 2711, 156, 15397, 8122, 111, 562, 19, 18120, 953, 571, 3, 4, 504, 43], [42, 129, 34129, 12, 29, 289, 251, 5, 371, 306, 14, 7128, 10, 94, 14104, 5, 51, 68, 483, 217, 1104, 34129, 452, 19, 2, 262, 175, 111214, 1343, 7, 12596, 3, 6400, 1713, 6, 1544, 7, 251, 371, 17, 563, 11, 14, 4, 237, 251, 351, 87, 2174, 4, 46, 193, 64, 57, 144, 24, 4, 254, 9, 4, 18154, 351, 10587, 59, 21, 60, 3745, 6, 126, 3, 3085, 2, 1428, 6, 1544, 7, 101, 45, 5, 20, 1517, 30, 39, 1178, 509, 20, 2, 1047, 204, 1882, 9, 251, 371, 5, 4, 1489, 6, 1612, 683, 6017, 34129, 11553, 2, 55768, 973, 8486, 1145, 193, 13142, 55768, 2443, 204669, 959, 5, 1440, 467, 12, 251, 1823, 7, 2, 314, 187, 9, 2227, 5, 9749, 11, 190, 3, 273, 36, 2, 251, 6, 40, 1696, 51, 68, 290, 6, 2, 5744, 228, 1059, 27, 34129, 1104, 2, 3325, 314, 371, 973, 1036, 3128, 371, 3, 929, 33, 737, 2, 2745, 2, 5120, 1741, 234, 59, 14, 1809, 53, 2352, 8112, 5, 59, 14, 1074, 164, 19, 12058, 5, 54974, 4684, 34129, 12, 4, 870, 4593, 306, 7, 2, 314, 1466, 780, 1982, 2247, 2604, 4150, 12400, 5, 2544, 4237, 34129, 289, 3532, 251, 1218, 3, 28512, 7, 69, 6753, 675, 176552, 2, 34129, 467, 7, 1466, 23, 44, 217, 18699, 1104, 33, 22, 1020, 3, 4, 1207, 6, 392, 98, 515, 5394, 5, 7761, 515, 6396, 953, 571, 3, 4, 504, 43], [42, 129, 44, 1008, 2223, 94, 2697, 5, 1984, 2097, 39, 243, 354, 339, 104, 1390, 946, 7, 1345, 1115, 136, 3, 435, 21, 1008, 5, 2953, 277, 145, 1008, 1094, 22, 82, 236, 7, 2, 1860, 7, 1115, 2, 445, 1703, 7, 105489, 142218, 7, 72495, 2130, 101, 1984, 2097, 243, 29, 1008, 15717, 2, 1008, 1404, 1654, 15, 2, 817, 46, 84, 15717, 29996, 3586, 29714, 6, 1766, 1289, 14, 2, 718, 1610, 2905, 201, 35, 12, 2, 1277, 1008, 9258, 3, 1567, 7, 2, 2339, 1860, 2, 560, 7, 48, 683, 29714, 236, 100, 2983, 7, 2, 65, 302, 282, 5, 8, 12533, 20, 1390, 401, 1828, 946, 2, 346589, 8, 5910, 29, 915, 3, 521, 29, 1247, 1984, 1772, 350, 2259, 7, 72495, 2130, 45, 2, 2223, 8, 335, 20, 495, 1603, 383, 472, 6, 2, 1404, 596, 399, 472, 11142, 13943, 3157, 15, 15698, 289, 271, 2854, 596, 15, 4, 1432, 252, 6, 1984, 6341, 39, 243, 53, 1487, 19, 177, 6, 2, 915, 1984, 6341, 11553, 2, 17799, 48, 4864, 332, 282, 4718, 4786, 65632, 13029, 400, 15, 17, 12, 34, 281, 113, 2, 2697, 58, 200, 5, 4, 299, 236, 275, 49, 102, 168, 11, 2, 6341, 1378, 1720, 13029, 1164, 21, 2, 4432, 7, 48, 2, 2339, 2697, 58, 200, 5, 44, 299, 39, 109, 36, 2, 645, 13029, 763, 60, 1279, 3, 1502, 5075, 6, 3031, 3511, 532, 953, 571, 3, 4, 504, 43]]\n"
     ]
    }
   ],
   "source": [
    "x_train_seq2 = []\n",
    "x_train2 =[]\n",
    "for i in x_train_seq:\n",
    "    if len(i) <250 :\n",
    "         x_train_seq2.append(i)\n",
    "\n",
    "print(len(x_train_seq2))\n",
    "Nb_token = [len(seq) for seq in x_train_seq2]\n",
    "print(max(Nb_token))\n",
    "print(x_train_seq2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba533733",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae4a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [s.split() for s in texts_for_fit_x]\n",
    "word2vec = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "embedding_dim = 100  \n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(num_word_token, len(word_index)) + 1\n",
    "\n",
    "# Matrice initialis√©e √† z√©ro\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Remplir la matrice avec les vecteurs Word2Vec\n",
    "for word, i in word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    if word in word2vec.wv:\n",
    "        embedding_matrix[i] = word2vec.wv[word]\n",
    "    else:\n",
    "        # Sinon, laisse un vecteur nul (ou tu peux mettre du bruit al√©atoire)\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "001201da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[-4.92345572e+00  7.00824082e-01  6.89447641e+00 -1.69778454e+00\n",
      " -1.98331928e+00  1.00524592e+00 -2.15554476e+00 -2.09611154e+00\n",
      "  2.70765205e-03 -1.44053653e-01  2.30726004e+00  1.75514603e+00\n",
      " -1.90051985e+00  5.27853870e+00  1.48908424e+00 -6.29056597e+00\n",
      " -5.41666985e+00 -2.59624076e+00 -3.68269491e+00  9.63919982e-02\n",
      " -4.05941105e+00 -3.91365051e+00  4.85412312e+00 -1.62773871e+00\n",
      "  1.07523906e+00  7.09555209e-01  1.19434893e+00  3.07036161e+00\n",
      "  2.68337607e+00  1.19562280e+00  2.00057435e+00  3.50957721e-01\n",
      " -3.33754897e+00  4.75333273e-01 -4.35329229e-02  3.03540683e+00\n",
      " -1.35771430e+00 -4.77833420e-01  4.51404953e+00  5.12701225e+00\n",
      " -4.12617445e+00 -2.84520268e+00  3.34777784e+00 -2.57369187e-02\n",
      "  3.91687441e+00  3.87300682e+00  1.50426126e+00 -5.03843546e+00\n",
      " -3.81812429e+00 -2.44377255e+00  2.14896107e+00 -5.36380434e+00\n",
      "  9.21726763e-01 -1.27962029e+00 -3.37533474e+00  9.32780683e-01\n",
      "  6.46621406e-01  2.25621057e+00  2.78801955e-02  1.58035040e+00\n",
      " -3.27982950e+00  3.09672922e-01 -4.07107115e+00 -3.84300947e+00\n",
      "  4.71343565e+00 -2.70789051e+00  3.63294125e+00  2.80572891e+00\n",
      "  4.11823082e+00  1.42903760e-01  1.24462712e+00 -2.44229531e+00\n",
      " -5.48052073e+00 -1.02781343e+00 -2.74883863e-03 -7.99892247e-01\n",
      " -1.87590793e-01 -9.98875856e-01 -1.12644351e+00  6.21820271e-01\n",
      " -2.07458711e+00 -3.27002311e+00 -3.13628626e+00 -2.82295924e-02\n",
      "  5.55355740e+00  6.64521217e-01 -6.81120157e-01 -1.81698775e+00\n",
      " -1.02344537e+00 -2.60170770e+00  5.26118422e+00 -3.19198775e+00\n",
      " -2.79963875e+00  2.73686385e+00 -3.54643869e+00  2.49104524e+00\n",
      " -6.30544662e-01 -9.54117656e-01 -1.85819519e+00  2.56907701e-01]\n",
      "(500001, 100)\n"
     ]
    }
   ],
   "source": [
    "#Exemple d'un token avec √ßa correspondance en vecteur\n",
    "print(tokenizer.word_index.get('to'))\n",
    "print(embedding_matrix[3])\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4c10f",
   "metadata": {},
   "source": [
    "### Padding\n",
    "Pas directement utile pour l'entrainnement (car on va refaire un padding pour chaque √©l√©ment du mod√®le) mais pour les contr√¥les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59823293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    42    627    129   1764    156   7723   9771     41   1864     19\n",
      "   2201    156   4981    395     96    156   1664     12 108176      9\n",
      "      4   3414  37980    295    150    400   1910   1445     15    339\n",
      "   1664     12   1938      3     22      2    515   2996    474      3\n",
      "    137     52    194    134    194    282     18      2   1383   6440\n",
      "     18   1374    330   3999   1445     15   9143     65  37980      8\n",
      "      7    540   2167      5     75  18071     39    109   1445     15\n",
      "      2   3329   1230     32   4218      4   4944   2996      7     52\n",
      "    182     74      2   2996     41     24  11061     20    511   1244\n",
      "  79420      5   2553     20      4  64342    162     21      2    237\n",
      "   5119    515    618      7  24681   3999   1445     15      4  37980\n",
      "     12      2    115   3856    823      9  14001    619  41746    619\n",
      "      5  60936    495  28211      6   2616     11     66    370  14174\n",
      "    136      3      2  12987   3102    495  60936    120     24   1340\n",
      "    104      2   2996   1445     15     11      8      2    215     45\n",
      "   1664     32  61472     81   1505    156   1445    454     12   5278\n",
      "   5250      9    619     11    452      7     16  14001      5   1631\n",
      "      3     16   4857   1445     73   1402     13     32      4  19339\n",
      "   2178   1938    193    339    524   1445    886     52   9143   2996\n",
      "      5     16    207  14001    619   2346      2    156    805      3\n",
      "   3009   2661      3    246   2372   1445     15      2    286    619\n",
      "   1275   8759     11     59    266    178    649   3982     53   2125\n",
      "   1780    475   9485      9   1573      6  36432    619     18    508\n",
      "    535    953    571      3      4    504     43      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0]\n",
      "249\n",
      "13704\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train_padded = pad_sequences(x_train_seq2, maxlen =249, padding = 'post', value = 0 )\n",
    "print(x_train_padded[0])\n",
    "print(len(x_train_padded[0]))\n",
    "print(len(x_train_padded))\n",
    "print(x_train_padded.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bfa7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# 1. S√©quences source (encodeur) ‚Äî sans <sos> ni <eos>\n",
    "x_train_seq_without_spetk = [\n",
    "    [elem for elem in seq if elem not in (42, 43)]\n",
    "    for seq in x_train_seq2\n",
    "]\n",
    "# encoder_input_seq = tokenizer.texts_to_sequences(x_train_seq_without_spetk)\n",
    "encoder_input_seq = pad_sequences(x_train_seq_without_spetk,maxlen =249, padding = 'post', value = 0 )\n",
    "\n",
    "# 2. D√©codeur - entr√©e (avec <sos>, sans <eos>)\n",
    "decoder_input_seq = tokenizer.texts_to_sequences([\"<sos> \" + s for s in y_train])\n",
    "decoder_input_seq = pad_sequences(decoder_input_seq,maxlen =249, padding = 'post', value = 0 )\n",
    "\n",
    "# 3. D√©codeur - cible (sans <sos>, avec <eos>)\n",
    "decoder_target_seq = tokenizer.texts_to_sequences([s + \" <eos>\" for s in y_train])\n",
    "decoder_target_seq = pad_sequences(decoder_target_seq,maxlen =249, padding = 'post', value = 0 )\n",
    "\n",
    "# 4. Adapter au format attendu (shape = (batch, seq_len, 1))\n",
    "decoder_target_seq = np.expand_dims(decoder_target_seq, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396bac6",
   "metadata": {},
   "source": [
    "### Cr√©ation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc8c1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
    "\n",
    "# --- Param√®tres\n",
    "latent_dim = 249\n",
    "vocab_size = embedding_matrix.shape[0]\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "max_seq_len = x_train_padded.shape[1]\n",
    "\n",
    "# --- Encoder\n",
    "encoder_inputs = Input(shape=(max_seq_len,))\n",
    "encoder_embedding_layer = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_seq_len,\n",
    "    trainable=True #Fine-tune l'embedding afin de l'adapter au mieux au data d'entrainnement\n",
    ")\n",
    "\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)#Pour ne pas perdre la r√©f√©rence de la couche\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# --- Decoder\n",
    "decoder_inputs = Input(shape=(None,))  # Attention : None pour pouvoir g√©n√©rer pas √† pas\n",
    "decoder_embedding_layer = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True #Fine-tune l'embedding afin de l'adapter au mieux au data d'entrainnement\n",
    ")\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs) #Pour ne pas perdre la r√©f√©rence de la couche\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# -- Attention (Luong-like avec multiplicative score)\n",
    "attention_layer = Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])  # shape: (batch, tgt_len, latent_dim)\n",
    "\n",
    "# -- Concatener contexte + sortie d√©codeur\n",
    "decoder_combined_context = Concatenate(axis=-1)([decoder_outputs, attention_outputs])\n",
    "\n",
    "# --- Dense de sortie\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')  # projection sur le vocab\n",
    "decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# --- Mod√®le final (entra√Ænement)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "# sparse_categorical_crossentropy (et pas categorical_crossentropy) car cible sont des entiers et non des vecteurs one-hot\n",
    "# On ajoute pas de metrics comme accuracy car elle serait calcul√© token par token ce qui est trompeur (pas bon KPI) car on veux √©valuer le sens global de la phrase pas juste correspondance des tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca39719",
   "metadata": {},
   "source": [
    "#### Entrainnement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m  84/1542\u001b[0m \u001b[32m‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m5:53:28\u001b[0m 15s/step - loss: 6.6857"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,              # Nombre d'√©poques sans am√©lioration avant arr√™t\n",
    "    restore_best_weights=True,  # Revenir aux meilleurs poids\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_seq2seq_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [encoder_input_seq, decoder_input_seq],\n",
    "    decoder_target_seq,\n",
    "    batch_size=8,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d10656",
   "metadata": {},
   "source": [
    "#### Importation mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea77851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_8       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ input_layer_9       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ embedding_8         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  ‚îÇ  <span style=\"color: #00af00; text-decoration-color: #00af00\">9,197,200</span> ‚îÇ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ embedding_9         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) ‚îÇ  <span style=\"color: #00af00; text-decoration-color: #00af00\">9,197,200</span> ‚îÇ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       ‚îÇ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>,      ‚îÇ    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> ‚îÇ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] ‚îÇ\n",
       "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       ‚îÇ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     ‚îÇ    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> ‚îÇ embedding_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      ‚îÇ            ‚îÇ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     ‚îÇ\n",
       "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      ‚îÇ            ‚îÇ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      ‚îÇ\n",
       "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ attention_4         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ lstm_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         ‚îÇ                   ‚îÇ            ‚îÇ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concatenate_4       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ lstm_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       ‚îÇ                   ‚îÇ            ‚îÇ attention_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">47,181,636</span> ‚îÇ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">91972</span>)            ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_8       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ input_layer_9       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ embedding_8         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m100\u001b[0m)  ‚îÇ  \u001b[38;5;34m9,197,200\u001b[0m ‚îÇ input_layer_8[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ embedding_9         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) ‚îÇ  \u001b[38;5;34m9,197,200\u001b[0m ‚îÇ input_layer_9[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mEmbedding\u001b[0m)         ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)       ‚îÇ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m,      ‚îÇ    \u001b[38;5;34m365,568\u001b[0m ‚îÇ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] ‚îÇ\n",
       "‚îÇ                     ‚îÇ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îÇ                     ‚îÇ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îÇ                     ‚îÇ \u001b[38;5;34m256\u001b[0m)]             ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)       ‚îÇ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     ‚îÇ    \u001b[38;5;34m365,568\u001b[0m ‚îÇ embedding_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      ‚îÇ            ‚îÇ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     ‚îÇ\n",
       "‚îÇ                     ‚îÇ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      ‚îÇ            ‚îÇ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      ‚îÇ\n",
       "‚îÇ                     ‚îÇ \u001b[38;5;34m256\u001b[0m)]             ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ attention_4         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ lstm_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mAttention\u001b[0m)         ‚îÇ                   ‚îÇ            ‚îÇ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concatenate_4       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ lstm_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mConcatenate\u001b[0m)       ‚îÇ                   ‚îÇ            ‚îÇ attention_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_4 (\u001b[38;5;33mDense\u001b[0m)     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      ‚îÇ \u001b[38;5;34m47,181,636\u001b[0m ‚îÇ concatenate_4[\u001b[38;5;34m0\u001b[0m]‚Ä¶ ‚îÇ\n",
       "‚îÇ                     ‚îÇ \u001b[38;5;34m91972\u001b[0m)            ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,307,172</span> (252.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,307,172\u001b[0m (252.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,307,172</span> (252.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,307,172\u001b[0m (252.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Charge le mod√®le entra√Æn√©\n",
    "model = save_model('best_seq2seq_model.h6', compile=False)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2853f4a",
   "metadata": {},
   "source": [
    "### Cr√©ation des mod√®les d'inf√©rences (encoder et d√©coder)\n",
    "(pour g√©n√©rer le texte par utilisation des poid du mod√®le pr√©c√©dement entrainn√©)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9821d6c",
   "metadata": {},
   "source": [
    "#### Mod√®le encodeur d‚Äôinf√©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# On suppose que le mod√®le complet a bien √©t√© charg√©\n",
    "# model = load_model(...)\n",
    "\n",
    "# R√©cup√©ration des bonnes entr√©es et couches\n",
    "encoder_inputs = model.input[0]\n",
    "\n",
    "# Tu r√©cup√®res directement les sorties de la couche LSTM\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.get_layer('lstm_8').output\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb42abd",
   "metadata": {},
   "source": [
    "#### Mod√®le d√©codeur d‚Äôinf√©rence avec attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "\n",
    "latent_dim = 256  # ou extrais-le du mod√®le\n",
    "decoder_inputs_single = Input(shape=(1,), name=\"decoder_input_infer\")\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name=\"input_h\")\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name=\"input_c\")\n",
    "decoder_hidden_state_input = Input(shape=(None, latent_dim), name=\"encoder_outputs\")\n",
    "\n",
    "# R√©cup√©ration des bonnes couches\n",
    "decoder_embedding_layer = model.get_layer('embedding_9')  # v√©rifie le nom exact\n",
    "decoder_lstm_layer = model.get_layer('lstm_9')\n",
    "attention_layer = model.get_layer('attention_4')\n",
    "dense_layer = model.get_layer('dense_4')\n",
    "\n",
    "# Embedding\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs_single)\n",
    "\n",
    "# LSTM\n",
    "decoder_outputs, state_h, state_c = decoder_lstm_layer(\n",
    "    decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "\n",
    "# Attention\n",
    "attention_out = attention_layer([decoder_outputs, decoder_hidden_state_input])\n",
    "decoder_combined_context = Concatenate(axis=-1)([decoder_outputs, attention_out])\n",
    "\n",
    "# Projection finale\n",
    "decoder_outputs = dense_layer(decoder_combined_context)\n",
    "\n",
    "# Mod√®le d'inf√©rence du d√©codeur\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs, state_h, state_c]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Mod√®le d'inf√©rence pour encoder = utiliser le m√™me que pour entrainnement\n",
    "# # Encoder d'inf√©rence (on donne une s√©quence source et on r√©cup√®re les √©tats)\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# # Entr√©es du d√©codeur\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_hidden_state_input = Input(shape=(max_seq_len, latent_dim))  # encoder_outputs\n",
    "\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_inputs_single = Input(shape=(1,))\n",
    "# decoder_embedding_inf = decoder_embedding_layer(decoder_inputs_single)\n",
    "\n",
    "# decoder_lstm_outputs, state_h_inf, state_c_inf = decoder_lstm(\n",
    "#     decoder_embedding_inf, initial_state=decoder_states_inputs\n",
    "# )\n",
    "\n",
    "# # Appliquer l'attention\n",
    "# attention_inf = attention_layer([decoder_lstm_outputs, decoder_hidden_state_input])\n",
    "# decoder_combined_context = Concatenate(axis=-1)([decoder_lstm_outputs, attention_inf])\n",
    "\n",
    "# # Projection finale\n",
    "# decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# # Mod√®le d'inf√©rence du d√©codeur complet\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs_single, decoder_hidden_state_input] + decoder_states_inputs,\n",
    "#     [decoder_outputs, state_h_inf, state_c_inf]\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21301292",
   "metadata": {},
   "source": [
    "### Fonction de g√©n√©ration de texte (green decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decode_sequence(input_seq, tokenizer, reverse_index, max_decoder_seq_len, start_token='sos', end_token='eos'):\n",
    "    # Encode l'entr√©e\n",
    "    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.array([[tokenizer.word_index[start_token]]])\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "\n",
    "    while not stop_condition and len(decoded_sentence) < max_decoder_seq_len:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs, state_h, state_c]\n",
    "        )\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_index.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_word == end_token or sampled_word == '':\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence.append(sampled_word)\n",
    "\n",
    "        target_seq = np.array([[sampled_token_index]])\n",
    "        state_h, state_c = h, c\n",
    "\n",
    "    return ' '.join(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85137308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_sequence(input_seq, tokenizer, max_target_len=50):\n",
    "#     # Encode la s√©quence d'entr√©e\n",
    "#     states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "#     # Token de d√©part (index de <sos>)\n",
    "#     sos_index = tokenizer.word_index['sos']\n",
    "#     eos_index = tokenizer.word_index['eos']\n",
    "\n",
    "#     target_seq = np.array([[sos_index]])\n",
    "\n",
    "#     decoded_sentence = []\n",
    "    \n",
    "#     for _ in range(max_target_len):\n",
    "#         output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "#         # Choisir le token avec la plus grande probabilit√©\n",
    "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         sampled_word = tokenizer.index_word.get(sampled_token_index, \"<unk>\")\n",
    "\n",
    "#         if sampled_word == 'eos':\n",
    "#             break\n",
    "\n",
    "#         decoded_sentence.append(sampled_word)\n",
    "\n",
    "#         # Le token g√©n√©r√© devient la prochaine entr√©e du d√©codeur\n",
    "#         target_seq = np.array([[sampled_token_index]])\n",
    "#         states_value = [h, c]\n",
    "\n",
    "#     return ' '.join(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637cedf",
   "metadata": {},
   "source": [
    "### Test d'utilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e233b8a",
   "metadata": {},
   "source": [
    "Faire attention a ce putain de troncage qui supprimer les eos avant l'entrainnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e7705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an interview hopefully none of you will be reading about it radcliffe's earnings from the first five potter films have been held in a trust fund which he has not been able to touch despite his growing fame and riches the actor says he is keeping his feet firmly on the ground people are always looking to say 'kid star goes off the rails ' he told reporters last month but i try very hard not to go that way because it would be too easy for them his latest outing as the boy wizard in harry potter and the order of the phoenix is breaking records on both sides of the atlantic and he will reprise the role in the last two films watch i reporter give her review of potter's latest ¬ª there is life beyond potter however the londoner has filmed a tv movie called my boy jack about author rudyard kipling and his son due for release later this year he will also appear in december boys an australian film about four boys who escape an orphanage earlier this year he made his stage debut playing a tortured teenager in peter shaffer's equus meanwhile he is braced for even closer media scrutiny now that he's legally an adult i just think i'm going to be more sort of fair game he told reuters e mail to a friend copyright 2007 reuters all rights reserved this material may not be published broadcast rewritten or redistributed eos\n",
      "[[    29    760   4070   2209      6     54     41     24   1912     52\n",
      "      17 100512   6444     21      2     72    182   6133   2507     22\n",
      "      38    424      7      4   1145   1749     48     13     23     34\n",
      "      38    328      3   1801    340     16   1047   3209      5  15626\n",
      "       2   1569     93     13     12   1784     16    949   6987     10\n",
      "       2    624     59     27    352    393      3    132 105486    384\n",
      "    1118    106      2  16607     37     13     73   1402     65    203\n",
      "      26     31    613    125    492     34      3    157     11    138\n",
      "      97     17     57     24    240   1461      9     77     16    721\n",
      "   10516     19      2    614  16112      7   1494   6133      5      2\n",
      "     603      6      2   4892     12   1843   1350     10    187   2021\n",
      "       6      2   3143      5     13     41  35164      2    657      7\n",
      "       2     65     58   2507    524     31   1011    378     25   1526\n",
      "       6  31135    721   2346     64     12    126   1625   6133    275\n",
      "       2  21822     23   2463      4    583   1306    173     83    614\n",
      "    1822     52   1966  44111  33428      5     16    296    507      9\n",
      "     851    193     35     46     13     41     56   1149      7    513\n",
      "    1232     29   1008    591     52    145   1232     33   1949     29\n",
      "   12620    343     35     46     13    107     16    960   2424    646\n",
      "       4   6686   1670      7   1255  94230 111204   1267     13     12\n",
      "   19576      9    131   2398    390   5530     82     11    584   4496\n",
      "      29   2280     31     70    165    408    149      3     24     51\n",
      "    1925      6   1815    255     13     73   6588    953    571      3\n",
      "       4    504   6394   1034   6588     60    522   4720     35   2104\n",
      "     120     34     24    257   2992  24441     53  30758     43]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, 500), found shape=(1, 249)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(decoded_input)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(input_seq)  \u001b[38;5;66;03m# Doit √™tre shape (1, seq_len)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m generated_summary = \u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_word\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mR√©sum√© g√©n√©r√© :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(generated_summary)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mdecode_sequence\u001b[39m\u001b[34m(input_seq, tokenizer, reverse_index, max_decoder_seq_len, start_token, end_token)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_sequence\u001b[39m(input_seq, tokenizer, reverse_index, max_decoder_seq_len, start_token=\u001b[33m'\u001b[39m\u001b[33msos\u001b[39m\u001b[33m'\u001b[39m, end_token=\u001b[33m'\u001b[39m\u001b[33meos\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Encode l'entr√©e\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     encoder_outputs, state_h, state_c = \u001b[43mencoder_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     target_seq = np.array([[tokenizer.word_index[start_token]]])\n\u001b[32m      8\u001b[39m     stop_condition = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim != dim:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    246\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m is \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    247\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mincompatible with the layer: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, 500), found shape=(1, 249)"
     ]
    }
   ],
   "source": [
    "# Suppose que tu veux g√©n√©rer √† partir de la premi√®re s√©quence x_test\n",
    "input_seq = x_train_padded[0:1]\n",
    "\n",
    "token_ids = input_seq[0]  # shape (seq_len,)\n",
    "# √âtape 2 : Reconvertir vers les mots\n",
    "reverse_index = tokenizer.index_word  # {index: word}\n",
    "decoded_input = ' '.join([reverse_index.get(idx, '') for idx in token_ids if idx != 0])\n",
    "print(decoded_input)\n",
    "\n",
    "print(input_seq)  # Doit √™tre shape (1, seq_len)\n",
    "generated_summary = decode_sequence(input_seq, tokenizer,tokenizer.index_word,100)\n",
    "\n",
    "print(\"R√©sum√© g√©n√©r√© :\")\n",
    "print(generated_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48834d10",
   "metadata": {},
   "source": [
    "## Tentative foir√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71cefa2",
   "metadata": {},
   "source": [
    "Explication choix de la structure :\n",
    "- Tokenisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1feda42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "1\n",
      "None\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 93\u001b[0m\n\u001b[0;32m     88\u001b[0m decoder_model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[0;32m     89\u001b[0m     [decoder_single_input, decoder_state_input_h, decoder_state_input_c],\n\u001b[0;32m     90\u001b[0m     [dec_out2, state_h2, state_c2]\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# --- 7. Entra√Ænement du mod√®le ---\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m     99\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# --- 8. Fonction d'inf√©rence (g√©n√©ration autonome) ---\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_sequence\u001b[39m(input_seq, max_len\u001b[38;5;241m=\u001b[39mmax_len_y):\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[0;32m    916\u001b[0m           bound_args\n\u001b[0;32m    917\u001b[0m       )\n\u001b[0;32m    918\u001b[0m   )\n\u001b[1;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[0;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\conda\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# # --- 1. Tokenisation \n",
    "# num_word_token = 15000\n",
    "# tokenizer = Tokenizer(num_words=num_word_token, oov_token=\"<unk>\")\n",
    "# tokenizer.fit_on_texts(mini_x_train + [\"<sos> \" + s + \" <eos>\" for s in mini_y_train])\n",
    "\n",
    "# encoder_input = tokenizer.texts_to_sequences(mini_x_train)\n",
    "# decoder_input = tokenizer.texts_to_sequences([\"<sos> \" + s for s in mini_y_train])\n",
    "# decoder_target = tokenizer.texts_to_sequences([s + \" <eos>\" for s in mini_y_train])\n",
    "\n",
    "# encoder_input_test = tokenizer.texts_to_sequences(mini_x_test)\n",
    "# decoder_input_test = tokenizer.texts_to_sequences([\"<sos> \" + s for s in mini_y_test])\n",
    "# decoder_target_test = tokenizer.texts_to_sequences([s + \" <eos>\" for s in mini_y_test])\n",
    "\n",
    "\n",
    "# print(tokenizer.word_index.get('<sos>'))\n",
    "# print(tokenizer.word_index.get('<eos>'))\n",
    "# print(tokenizer.word_index.get('<unk>'))\n",
    "# print(tokenizer.word_index.get('<pad>'))\n",
    "\n",
    "# # --- 2. Padding \n",
    "# max_len_x = max(len(s) for s in encoder_input)\n",
    "# max_len_y = max(len(s) for s in decoder_input)\n",
    "\n",
    "# encoder_input = pad_sequences(encoder_input, maxlen=max_len_x, padding='post')\n",
    "# decoder_input = pad_sequences(decoder_input, maxlen=max_len_y, padding='post')\n",
    "# decoder_target = pad_sequences(decoder_target, maxlen=max_len_y, padding='post')\n",
    "\n",
    "# encoder_input_test = pad_sequences(encoder_input_test, maxlen=max_len_x, padding='post')\n",
    "# decoder_input_test = pad_sequences(decoder_input_test, maxlen=max_len_y, padding='post')\n",
    "# decoder_target_test = pad_sequences(decoder_target_test, maxlen=max_len_y, padding='post')\n",
    "\n",
    "# # --- 3. Hyperparam√®tres ---\n",
    "# dim_embedding = 150\n",
    "# lstm_units = 256\n",
    "\n",
    "# # --- 4. Mod√®le d'entra√Ænement (encodeur + d√©codeur) ---> teacher forcing\n",
    "\n",
    "# # Encodeur\n",
    "# enc_in = Input(shape=(max_len_x,), name='encoder_input')\n",
    "# enc_emb = Embedding(num_word_token, dim_embedding, name='encoder_embedding')(enc_in)\n",
    "# enc_out, state_h, state_c = LSTM(lstm_units, return_state=True, name='encoder_lstm')(enc_emb)\n",
    "\n",
    "# # D√©codeur (entr√©e)\n",
    "# dec_in = Input(shape=(max_len_y,), name='decoder_input')\n",
    "# dec_emb = Embedding(num_word_token, dim_embedding, name='decoder_embedding')(dec_in)\n",
    "\n",
    "# # D√©codeur LSTM avec initialisation des √©tats par l'encodeur\n",
    "# dec_lstm, _, _ = LSTM(\n",
    "#     lstm_units, return_sequences=True, return_state=True, name='decoder_lstm')(\n",
    "#         dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# # Couche dense pour pr√©diction sur vocabulaire\n",
    "# dec_out = Dense(num_word_token, activation='softmax', name='decoder_dense')(dec_lstm)\n",
    "\n",
    "# # Mod√®le complet pour entra√Ænement avec teacher forcing\n",
    "# model = Model([enc_in, dec_in], dec_out)\n",
    "# model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "# # --- 5. Mod√®le encodeur pour inf√©rence ---> pour g√©n√©rer les √©tats cach√©s\n",
    "\n",
    "# # Ce mod√®le prend la s√©quence source et retourne les √©tats cach√©s\n",
    "# encoder_model = Model(enc_in, [state_h, state_c])\n",
    "\n",
    "# # --- 6. Mod√®le d√©codeur pour inf√©rence ---\n",
    "\n",
    "# # Entr√©es pour un seul token + √©tats cach√©s initiaux\n",
    "# decoder_state_input_h = Input(shape=(lstm_units,), name='decoder_state_input_h')\n",
    "# decoder_state_input_c = Input(shape=(lstm_units,), name='decoder_state_input_c')\n",
    "# decoder_single_input = Input(shape=(1,), name='decoder_single_input')  # un mot √† la fois\n",
    "\n",
    "# # Embedding du token unique\n",
    "# dec_emb2 = Embedding(num_word_token, dim_embedding, name='decoder_embedding')(decoder_single_input)\n",
    "\n",
    "# # LSTM pour un pas de temps, initialis√© avec les √©tats pr√©c√©dents\n",
    "# dec_lstm2, state_h2, state_c2 = LSTM(\n",
    "#     lstm_units, return_sequences=True, return_state=True, name='decoder_lstm'\n",
    "# )(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# # Couche dense pour pr√©dire le prochain mot\n",
    "# dec_out2 = Dense(num_word_token, activation='softmax', name='decoder_dense')(dec_lstm2)\n",
    "\n",
    "# # Mod√®le d√©codeur inf√©rence\n",
    "# decoder_model = Model(\n",
    "#     [decoder_single_input, decoder_state_input_h, decoder_state_input_c],\n",
    "#     [dec_out2, state_h2, state_c2]\n",
    "# )\n",
    "# # --- 7. Entra√Ænement du mod√®le ---\n",
    "# model.fit(\n",
    "#     [encoder_input, decoder_input],\n",
    "#     np.expand_dims(decoder_target, -1),\n",
    "#     batch_size=64,\n",
    "#     epochs=5,\n",
    "#     validation_split=0.1\n",
    "# )\n",
    "\n",
    "# # --- 8. Fonction d'inf√©rence (g√©n√©ration autonome) ---\n",
    "# def decode_sequence(input_seq, max_len=max_len_y):\n",
    "#     states_value = encoder_model.predict(input_seq)\n",
    "#     sos_token_index = tokenizer.word_index.get('<sos>')\n",
    "#     if sos_token_index is None:\n",
    "#         raise ValueError(\"Le tokenizer doit contenir le token <sos>\")\n",
    "#     target_seq = np.array([[sos_token_index]])\n",
    "\n",
    "#     stop_condition = False\n",
    "#     decoded_sentence = []\n",
    "\n",
    "#     state_h, state_c = states_value\n",
    "\n",
    "#     for _ in range(max_len):\n",
    "#         output_tokens, h, c = decoder_model.predict([target_seq, state_h, state_c])\n",
    "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         sampled_word = tokenizer.index_word.get(sampled_token_index, '<unk>')\n",
    "\n",
    "#         if sampled_word == '<eos>' or sampled_word == '<pad>':\n",
    "#             break\n",
    "\n",
    "#         decoded_sentence.append(sampled_word)\n",
    "#         target_seq = np.array([[sampled_token_index]])\n",
    "#         state_h, state_c = h, c\n",
    "\n",
    "#     return ' '.join(decoded_sentence)\n",
    "\n",
    "# # --- 8. Test de g√©n√©ration sur quelques exemples ---\n",
    "# for i in range(5):\n",
    "#     input_seq = encoder_input_test[i:i+1]\n",
    "#     print(f\"--- Exemple {i+1} ---\")\n",
    "#     print(\"Texte source :\", mini_x_test[i])\n",
    "#     print(\"R√©sum√© attendu :\", mini_y_test[i])\n",
    "#     print(\"R√©sum√© g√©n√©r√© :\", decode_sequence(input_seq))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ed0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 12s/step - accuracy: 0.2957 - loss: 9.1615 - val_accuracy: 0.3739 - val_loss: 6.2785\n",
      "Epoch 2/5\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 12s/step - accuracy: 0.3754 - loss: 5.7393 - val_accuracy: 0.3739 - val_loss: 5.4796\n",
      "Epoch 3/5\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 12s/step - accuracy: 0.3774 - loss: 5.1840 - val_accuracy: 0.3739 - val_loss: 5.2785\n",
      "Epoch 4/5\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 16s/step - accuracy: 0.3751 - loss: 4.9861 - val_accuracy: 0.3739 - val_loss: 5.1780\n",
      "Epoch 5/5\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 16s/step - accuracy: 0.3756 - loss: 4.8529 - val_accuracy: 0.3739 - val_loss: 5.0360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a714f15430>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(\n",
    "#     [encoder_input, decoder_input],\n",
    "#     np.expand_dims(decoder_target, -1),\n",
    "#     batch_size=64,\n",
    "#     epochs=5,\n",
    "#     validation_split=0.1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 0.4846 - loss: 3.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: 0.494242399930954\n",
      "Loss: 3.838369846343994\n"
     ]
    }
   ],
   "source": [
    "# #5. Pr√©diction et √©valuation\n",
    "# loss, metrics = model.evaluate(\n",
    "#     [encoder_input_test, decoder_input_test],\n",
    "#     np.expand_dims(decoder_target_test, -1)\n",
    "# )\n",
    "# if metrics:\n",
    "#     print(\"Metrics:\", metrics)\n",
    "#     print(\"Loss:\", loss)\n",
    "\n",
    "# model.save(r\"Model/DL/mon_modele.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7ef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte encod√© : [[   0    0    0 ...    0    0    0]\n",
      " [   5    0    0 ...    0    0    0]\n",
      " [1356    0    0 ...    0    0    0]\n",
      " ...\n",
      " [  50    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# texte = \"\"\"\n",
    "# Artificial intelligence is transforming the way we work and live. From smart assistants to automated medical diagnostics, AI technologies are becoming increasingly integrated into our daily lives. However, this rapid progress also raises important questions about privacy, job displacement, and the ethical use of data.\n",
    "# As AI continues to evolve, it is crucial for society to address these challenges and ensure that the benefits of AI are shared equitably. Policymakers, technologists, and the public must work together to create a framework that promotes innovation while safeguarding individual rights and societal values.\n",
    "# The future of AI holds great promise, but it is essential to navigate the complexities it brings. By fostering collaboration and open dialogue, we can harness the power of AI to improve our world while minimizing its risks.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# def encode_text(texte):\n",
    "#     inputs = tokenizer.texts_to_sequences(texte)\n",
    "#     inputs = pad_sequences(inputs, maxlen=max_len_x, padding='post')\n",
    "    \n",
    "#     return inputs\n",
    "\n",
    "# model.predict(encode_text(texte))\n",
    "\n",
    "\n",
    "    \n",
    "# # model.predict(texte)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPUVenv)",
   "language": "python",
   "name": "gpuvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
